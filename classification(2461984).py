# -*- coding: utf-8 -*-
"""classification(2461984).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQ2f93MPwG351kiH3mraod09xtJVngjR

1.**Exploratory Data Analysis & Data Understanding**

dataset name:adult census income dataset

The Adult Census Income dataset was created by Barry Becker in 1994 using data extracted from the 1994 U.S. Census database.

The dataset was accessed from Kaggle .

This dataset aligns with UN Sustainable Development Goal 8: Decent Work and Economic Growth, as it focuses on income classification and socio-economic factors such as education, occupation, working hours, and employment type. Understanding income inequality supports evidence-based economic policy and inclusive growth.

1.2.Exploratory Data Analysis (EDA):
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/drive/MyDrive/ai final assesment/adult.csv")  # adjust filename if needed
df.head()

df.shape

df.info()

df.describe()

df.isin(["?"]).sum()

df.replace("?", np.nan, inplace=True)
df.isnull().sum()

df.dropna(inplace=True)

df['income'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='income', data=df)
plt.title("Income Distribution")
plt.show()

df['income'].value_counts(normalize=True)

sns.boxplot(x='income', y='age', data=df)
plt.title("Age vs Income")
plt.show()

pd.crosstab(df['education'], df['income'])

pd.crosstab(df['education'], df['income']).plot(kind='bar', stacked=True)
plt.title("Education vs Income")
plt.show()

sns.histplot(df['hours.per.week'], bins=30, kde=True)
plt.title("Working Hours per Week")
plt.show()

# Identify numeric columns
numeric_cols = df.select_dtypes(include=np.number).columns

# Compute correlation matrix
corr = df[numeric_cols].corr()
# Set up the figure
plt.figure(figsize=(12,8))
# Create heatmap with enhancements
sns.heatmap(
    corr,
    annot=True,            # show correlation values
    fmt=".2f",             # format values to 2 decimal places
    cmap="coolwarm",       # color scheme
    cbar=True,             # show color bar
    square=True,           # make cells square
    linewidths=0.5,        # add lines between cells
    annot_kws={"size":10}  # adjust annotation font size
)

# Add title with styling
plt.title("Feature Correlation Heatmap (Numeric Columns)", fontsize=16, pad=20)

# Improve layout
plt.tight_layout()
plt.show()

# Target variable
y = df['income']

# Features
X = df.drop('income', axis=1)
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = le.fit_transform(y)  # <=50K → 0, >50K → 1
X = pd.get_dummies(X, drop_first=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,  # keeps class distribution
    random_state=42
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""2. Build a Neural Network Model"""

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(
    hidden_layer_sizes=(64, 32),
    activation='relu',
    solver='adam',
    max_iter=500,
    early_stopping=True,
    random_state=42
)

mlp.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Predictions
y_pred = mlp.predict(X_test_scaled)
y_prob = mlp.predict_proba(X_test_scaled)[:,1]

# Metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_prob))

fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr, label="Neural Network")
plt.plot([0,1],[0,1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

"""3.Build a Primary Model"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""primary model-1 :logistic regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(
    max_iter=1000,
    class_weight='balanced',  # handle imbalance
    random_state=42
)
lr.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

y_pred_lr = lr.predict(X_test_scaled)
y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]

print("Logistic Regression Metrics")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Precision:", precision_score(y_test, y_pred_lr))
print("Recall:", recall_score(y_test, y_pred_lr))
print("F1 Score:", f1_score(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, y_prob_lr))

"""primary model-2 Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(
    max_depth=10,           # limit depth to avoid overfitting
    class_weight='balanced',
    random_state=42
)
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)
y_prob_dt = dt.predict_proba(X_test)[:, 1]

print("Decision Tree Metrics")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt))
print("Recall:", recall_score(y_test, y_pred_dt))
print("F1 Score:", f1_score(y_test, y_pred_dt))
print("ROC-AUC:", roc_auc_score(y_test, y_prob_dt))

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Decision Tree"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_lr),
        accuracy_score(y_test, y_pred_dt)
    ],
    "Precision": [
        precision_score(y_test, y_pred_lr),
        precision_score(y_test, y_pred_dt)
    ],
    "Recall": [
        recall_score(y_test, y_pred_lr),
        recall_score(y_test, y_pred_dt)
    ],
    "F1 Score": [
        f1_score(y_test, y_pred_lr),
        f1_score(y_test, y_pred_dt)
    ]
})

results

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm_lr = confusion_matrix(y_test, y_pred_lr)
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['<=50K', '>50K'])
disp_lr.plot(cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

cm_dt = confusion_matrix(y_test, y_pred_dt)
disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['<=50K', '>50K'])
disp_dt.plot(cmap='Blues')
plt.title("Confusion Matrix - Decision Tree")
plt.show()

"""4. Hyper-parameter Optimization with Cross-Validation

"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid_lr = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2'],        # l2 is standard
    'solver': ['liblinear', 'saga']
}

grid_lr = GridSearchCV(
    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),
    param_grid=param_grid_lr,
    cv=5,
    scoring='f1',   # Use F1 because dataset is imbalanced
    n_jobs=-1
)

grid_lr.fit(X_train_scaled, y_train)

print("Best hyperparameters for Logistic Regression:", grid_lr.best_params_)
print("Best cross-validation F1 score:", grid_lr.best_score_)

from sklearn.tree import DecisionTreeClassifier

param_grid_dt = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(class_weight='balanced', random_state=42),
    param_grid=param_grid_dt,
    cv=5,
    scoring='f1',
    n_jobs=-1
)

grid_dt.fit(X_train, y_train)

print("Best hyperparameters for Decision Tree:", grid_dt.best_params_)
print("Best cross-validation F1 score:", grid_dt.best_score_)

"""5. Feature Selection

Feature Selection for Logistic Regression
"""

from sklearn.feature_selection import SelectKBest, mutual_info_classif

# Keep top 20 features
selector = SelectKBest(score_func=mutual_info_classif, k=20)
selector.fit(X_train_scaled, y_train)

# Get selected features
selected_features_filter = X_train.columns[selector.get_support()]
print("Selected features (Filter method):", selected_features_filter)

# Transform train/test sets
X_train_sel = selector.transform(X_train_scaled)
X_test_sel = selector.transform(X_test_scaled)

"""We used a filter-based feature selection method with Mutual Information because it selects the most relevant features by measuring their relationship with the target variable, including non-linear patterns, while reducing the number of features and improving model efficiency.

Feature Selection for Decision Tree (Embedded)
"""

# Use best Decision Tree from GridSearchCV
best_dt = grid_dt.best_estimator_

# Feature importance
importances = best_dt.feature_importances_
feature_importances = pd.Series(importances, index=X_train.columns)
selected_features_dt = feature_importances.nlargest(20).index  # top 20 features
print("Selected features for Decision Tree:", selected_features_dt)

"""An embedded feature selection method was used for the Decision Tree model. Decision Trees naturally evaluate the importance of each feature during training based on how much it reduces impurity at each split."""

# Logistic Regression (Filter-selected features)
X_train_lr_sel = X_train_scaled[:, selector.get_support()]  # selector from SelectKBest
X_test_lr_sel = X_test_scaled[:, selector.get_support()]

# Decision Tree (Embedded-selected features)
X_train_dt_sel = X_train[selected_features_dt]  # selected_features_dt from Task 5
X_test_dt_sel = X_test[selected_features_dt]

"""6.Final Models and Comparative Analysis"""

# Logistic Regression (Filter-selected features)
best_lr = grid_lr.best_estimator_  # best hyperparameters from Task 4

# Train
best_lr.fit(X_train_lr_sel, y_train)

# Predict
y_pred_lr = best_lr.predict(X_test_lr_sel)
y_prob_lr = best_lr.predict_proba(X_test_lr_sel)[:, 1]

# Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

acc_lr = accuracy_score(y_test, y_pred_lr)
prec_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
roc_lr = roc_auc_score(y_test, y_prob_lr)

print("Logistic Regression Metrics:")
print(f"Accuracy: {acc_lr:.3f}")
print(f"Precision: {prec_lr:.3f}")
print(f"Recall: {recall_lr:.3f}")
print(f"F1-Score: {f1_lr:.3f}")
print(f"ROC-AUC: {roc_lr:.3f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm_lr = confusion_matrix(y_test, y_pred_lr)
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['<=50K','>50K'])
disp_lr.plot(cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_estimator(best_lr, X_test_lr_sel, y_test)
plt.title("ROC Curve - Logistic Regression")
plt.show()

# Decision Tree (Embedded-selected features)
best_dt = grid_dt.best_estimator_  # best hyperparameters from Task 4

# Train
best_dt.fit(X_train_dt_sel, y_train)

# Predict
y_pred_dt = best_dt.predict(X_test_dt_sel)
y_prob_dt = best_dt.predict_proba(X_test_dt_sel)[:, 1]

# Metrics
acc_dt = accuracy_score(y_test, y_pred_dt)
prec_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
roc_dt = roc_auc_score(y_test, y_prob_dt)

print("Decision Tree Metrics:")
print(f"Accuracy: {acc_dt:.3f}")
print(f"Precision: {prec_dt:.3f}")
print(f"Recall: {recall_dt:.3f}")
print(f"F1-Score: {f1_dt:.3f}")
print(f"ROC-AUC: {roc_dt:.3f}")

cm_dt = confusion_matrix(y_test, y_pred_dt)
disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['<=50K','>50K'])
disp_dt.plot(cmap='Blues')
plt.title("Confusion Matrix - Decision Tree")
plt.show()

RocCurveDisplay.from_estimator(best_dt, X_test_dt_sel, y_test)
plt.title("ROC Curve - Decision Tree")
plt.show()

import pandas as pd

comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Decision Tree'],
    'Features Used': ['Filter-selected (20)', 'Embedded-selected (20)'],
    'CV Score (F1)': [grid_lr.best_score_, grid_dt.best_score_],
    'Test Accuracy': [acc_lr, acc_dt],
    'Test Precision': [prec_lr, prec_dt],
    'Test Recall': [recall_lr, recall_dt],
    'Test F1-Score': [f1_lr, f1_dt],
    'Test ROC-AUC': [roc_lr, roc_dt]
})

comparison